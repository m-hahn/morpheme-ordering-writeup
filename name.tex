\documentclass[11pt,letterpaper]{article}

\usepackage{showlabels}
\usepackage{fullpage}
\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
%\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{lingmacros}

\usepackage{natbib}
\bibliographystyle{unsrtnat}

%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{bm}
\usepackage{graphicx}
%\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{multirow}


%\usepackage{linguex}
%\usepackage{lingmacros}


\usepackage{hyperref}

\usepackage{tikz-dependency}
\usepackage{changepage}
\usepackage{longtable}


\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\Prob}[0]{\mathbb{P}}
\newcommand{\Ff}[0]{\mathcal{F}}

\usepackage{multirow}

\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\mhahn[1]{{\color{red}(#1)}}
\newcommand\note[1]{{\color{red}(#1)}}
\newcommand\jd[1]{{\color{red}(#1)}}
\newcommand\rljf[1]{{\color{red}(#1)}}
\newcommand{\key}[1]{\textbf{#1}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathop{\mathbb{E}}}



\usepackage{amsthm}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}

\newcounter{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{thm}[theorem]{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{defin}[theorem]{Definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}


%\usepackage{linguex}
%\newcommand{\key}[1]{\textbf{#1}}



\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thesection}{S\arabic{section}}

\newcommand{\utterance}{\mathcal{U}}
\newcommand{\tree}{\mathcal{T}}



\usepackage{siunitx}



\usepackage{longtable}



\frenchspacing
%\def\baselinestretch{0.975}

%\emnlpfinalcopy
%\def\emnlppaperid{496}

\title{Morpheme Ordering across Languages reflects optimization for memory efficiency / information locality}

\begin{document}

\maketitle

\begin{abstract}
    The order of morphemes in a word shows well-documented tendencies across languages.
    These have been explained in terms of notions such as semantic scope and relevance in prior work.
A recent theory (CITE) argues that word and morpheme order in language optimizes a tradeoff between Memory and surprisal, and provided initial evidence from twoi languages that morpheme order can partly be explained by optimization for this treadeoff.
    %Efficient memory--surprisal tradeoffs are achieved by orders that display information locality, whereby elements that have high mutual information are closer together.
    In this work, we test this idea more extensively using data from four additional agglutinative languages with significant amounts of morphology.
    
%    In this work, we examine corpus data from four languages to show that optimizing for tradeoff efficiency mostly predicts morpheme order in verb and noun morphology.
    
\end{abstract}



\section{Introduction}

Across languages, words are composed of morphemes, commonly defined as the smallest meaning-bearing units of language.
Morphemes can take different shapes.
In many cases, words can be segmented into a sequence of morphemes, as in the English word runners (root run-, derivation -er-, plural -s).
The order of morphemes within a word follows well-documented cross-linguistic tendencies, for instance, plural markers tend to be closer to noun stems than case markers \citep[112]{greenberg1963universals}.
These tendencies have been explained in terms of semantic scope \citep{bybee-morphology-1985}


One family of explanations holds that those affixes are closest to the root that are most relevant to the root (CITE).
Other explanations suggest that morphemes are ordered based on (TODO CITE).

A recent theory proposes that ordering universals in language can be understood as arising from optimization of processing effort under memory limitations (CITE).
Formally, they introduced the notion of a memory-surprisal tradeoff: Depending on how many memory resources a comprehender invests, they can achieve different levels of surprisal.
(CITE) hypothesize that order of words and morphemes in language optimizes this tradeoff.
Optimizing the memory-surprisal tradeoff amounts to putting elements together when they predict each other strongly, as measured by mutual information.
In a case study, CITE apply this to morpheme ordering in Japanese and Sesotho, finding that optimization could partly reproduce morpheme ordering in these languages.
They also suggested that mutual information formalizes Bybee's notion of relevance.


In this work, we examine this theory on a broader basis by considering data from four additional languages (Korean, Turkish, Hungarian, Finnish).
We focus on the case where words are composed of a stem and some number of affixes that are concatenated.


Greenberg 1963:  "the expression of number almost always comes between the noun base and the expression of case" (Greenberg 1963:112)

%Scope, Syntax, History
%Another family of explanations holds that morpheme ordering is to be explained diachronically, and that morpheme order reflects the order of independent words in earlier stages of a language.
%- Relevance, Proximity Principle, Iconicity


We focus on languages where words tend to have multiple morphemes and these are mostly realized separately. Such languages are referred to as agglutinative (Greenberg 1954).


Bybee on explanation:
- regarding the historical explanation (Givon 1971, Vennemann 1973):
Morphology is not always fossilizes syntax (Bybee and Brewer 1980, Bybee 1985 p. 39--40)


\section{Morpheme Ordering and Ordering Universals}

Verbs

Nouns


\section{Background: Memory--Surprisal Tradeoff and Information Locality}

CITE introduced the Memory--Surprisal Tradoff as an architecture-independent formalization of memory efficiency in incremental processing.


We formalize these three observations into postulates intended to provide a simplified picture of what is known about online language comprehension. Consider a listener comprehending a sequence of words $w_1, \dots, w_t, \dots, w_n$, at an arbitrary time $t$.
\begin{enumerate}
    \item Comprehension Postulate 1 (Incremental memory). At time $t$, the listener has an incremental \key{memory state} $m_t$ that contains her stored information about previous words. The memory state is characterized by a \key{memory encoding function} $M$ such that $m_t = M(w_{t-1}, m_{t-1})$.
    \item Comprehension Postulate 2 (Incremental prediction). The listener has a subjective probability distribution at time $t$ over the next word $w_t$ as a function of the memory state $m_t$. This probability distribution is denoted $P(w_t|m_t)$.
    \item Comprehension Postulate 3 (Linking hypothesis). Processing a word $w_t$ incurs difficulty proportional to the \key{surprisal} of $w_t$ given the memory state $m_t$:
    \begin{equation}
    \label{eq:lossy-surp}
    \text{Difficulty} \propto -\log P(w_t | m_t).
\end{equation}
\end{enumerate}

\begin{definition}
The \key{memory--surprisal tradeoff curve} for a process $W$ is the lowest achievable average surprisal $S_M$ for each value of $H_M$. Let $R$ denote an upper bound on the memory entropy $H_M$; then the memory--surprisal tradeoff curve as a function of $R$ is given by
\begin{equation}
    \label{eq:ms-formal}
    D(R) \equiv \min_{M : H_M \le R} S_M,
\end{equation}
where the minimization is over all memory encoding functions $M$ whose entropy $H_M$ is less than or equal to $R$.
\end{definition}

Our argument will make use of a quantity called \key{mutual information}. Mutual information is the most general measure of statistical association between two random variables. The mutual information between two random variables $X$ and $Y$, conditional on a third random variable $Z$, is defined as:
\begin{align}
\label{eq:mi}
    \operatorname{I}[X:Y|Z] &\equiv \sum_{x,y,z} P(x,y,z) \log \frac{P(x,y|z)}{P(x|z)P(y|z)}. % \text{ bits} \\
    %\nonumber
    %&= \operatorname{H}[X|Z] - \operatorname{H}[X|Y,Z] \\
    %\nonumber
    %&= \operatorname{H}[Y|Z] - \operatorname{H}[Y|X,Z].
\end{align}
Mutual information is always non-negative. It is zero when $X$ and $Y$ are conditionally independent given $Z$, and positive whenever $X$ gives any information that makes the value of $Y$ more predictable, or vice versa. 

We will study the mutual information structure of natural language sentences, and in particular the mutual information between words at certain distances in linear order. We define the notation $I_t$ to mean the mutual information between words at distance $t$ from each other, conditional on the intervening words:
\begin{equation*}
    I_t \equiv \operatorname{I}[w_t : w_0 | w_1, \dots, w_{t-1}].
\end{equation*}
This quantity, visualized in Figure~\ref{fig:theorem}(a), measures how much predictive information is provided about the current word by the word $t$ steps in the past.
It is a statistical property of the language, and can be estimated from large-scale text data.

Equipped with this notion of mutual information at a distance, we can now state our theorem:
\begin{thm}\label{prop:suboptimal}(Information locality bound)
For any positive integer $T$, let $M$ be a memory encoding function such that
\begin{equation}
\label{eq:memory-bound}
H_M \le \sum_{t=1}^T t I_t.    
\end{equation}
Then we have a lower bound on the average surprisal under the memory encoding function $M$:
\begin{equation}
\label{eq:surprisal-bound}
S_M \ge S_\infty + \sum_{t=T+1}^\infty I_t.
\end{equation}
\end{thm}
\section{Methods}

\subsection{Data} % TODO: Becky
% why agglutinative?
We considered:

- UD data for Japanese, Korean, Turkish, Hungarian, Finnish

- CHILDES data for Sesotho

The Sesotho and Japanese data was previously used by CITE; we reanalyze these data here in a way consistent across all six languages.


\subsection{Nouns}

\paragraph{Turkish Nouns}
\begin{enumerate}
    \item Number
    \item Possessor number and possessor person
    \item Case 
\end{enumerate}

\paragraph{Hungarian Nouns}
\url{https://cl.lingfil.uu.se/~bea/publ/megyesi-hungarian.pdf}
\begin{enumerate}
    \item Number
    \item Possessor person
    \item Possessor number
    \item Case 
\end{enumerate}

\paragraph{Finnish Nouns}
\begin{enumerate}
    \item Derivation
    \item Number
    \item Case 
    \item Possessor person and possessor number
\end{enumerate}

\subsection{Verbs}

\paragraph{Hungarian Verbs}
\begin{enumerate}
    \item Voice
    \item Tense 
    \item Mood
    \item Definiteness, person, and number
\end{enumerate}

\paragraph{Turkish Verbs}
\begin{enumerate}
    \item Voice
    \item Negation / polarity
    \item Aspect
    \item Evidential 
    \item Tense 
    \item Mood 
    \item Person and Number
    \item Formality
\end{enumerate}

\paragraph{Finnish Verbs}
\begin{enumerate}
    \item Voice
    \item Tense 
    \item Mood 
    \item Person and number
    \item Clitic
\end{enumerate}

\paragraph{Sesotho Verbs}

\begin{enumerate}
    \item Subject agreement: This morpheme encodes agreement with the subject, for person, number, and noun class (the latter only in the 3rd person) \cite[\textsection 395]{doke1967textbook}.
            The annotation provided by \cite{demuth1992acquisition} distinguishes between ordinary subject agreement prefixes and agreement prefixes used in relative clauses; we distinguish these morpheme types here.

    \item Negation \citep[\textsection 429]{doke1967textbook}

    \item Tense/aspect marker   \citep[\textsection 400--424]{doke1967textbook}

    \item Object agreement or reflexive marker \citep[\textsection 459]{doke1967textbook}.
    Similar to subject agreement, object agreement denotes person, number, and noun class features of the object.
\end{enumerate}
We identified the following suffixes:

% something we might cite at some point (pointers from Beth Levine), about templatic morpheme order in Bantu
% Hyman2003 in literature.bib
% Jeffrey Good, Strong Linearity: Three Case Studies Towards a Theory of Morphosyntactic Templatic Constructions (Diss, 2003)

\begin{enumerate}
\item Semantic derivation: reversive (e.g., `do' $\rightarrow$ `undo') \citep[\textsection 345]{doke1967textbook}
\item Valence: Common valence-altering suffixes include causative, neuter/stative, applicative, and reciprocal \citep[\textsection 307--338]{doke1967textbook}. See SI for details on their meanings.
    \item Voice: passive \citep[\textsection 300]{doke1967textbook}
    \item Tense \citep[\textsection 369]{doke1967textbook}
    \item Mood \citep[\textsection 386--445]{doke1967textbook}
    \item Interrogative and relative markers, appended to verbs in certain interrogative and relative clauses \citep[\textsection 160, 271, 320, 714, 793]{doke1967textbook}.
    %-\textit{ng}.
    %The interrogative marker -\textit{ng} is an enclitized form of the interrogative `what'.
    %The relative marker -\textit{ng} is suffixed to verbs in relative clauses.
\end{enumerate}



\paragraph{Japanese Verbs}


\begin{enumerate}
\item \textit{suru}: obligatory suffix after Sino-Japanese words when they are used as verbs
\item Valence: causative (-\textit{ase}-) (\citet[142]{hasegawa2014japanese}, \citet[Chapter 13]{kaiser2013japanese})
\item Voice and Mood: passive (-\textit{are}-, -\textit{rare}-) (\citet[152]{hasegawa2014japanese}, \citet[Chapter 12]{kaiser2013japanese}) and potential (-\textit{e}-, -\textit{are}-, -\textit{rare}-) \citep[398]{kaiser2013japanese}
\item Politeness (-\textit{mas}-) \citep[190]{kaiser2013japanese}.
\item Mood: desiderative (-\textit{ta}-) \citep[238]{kaiser2013japanese}
\item Negation (-\textit{n}-)
\item Tense, Aspect, Mood, and Finiteness: past (-\textit{ta}), future/hortative (-\textit{yoo}) \citep[229]{kaiser2013japanese}, nonfiniteness (-\textit{te}) \citep[186]{kaiser2013japanese}
%\item Nonfiniteness: the suffix -\textit{te} derives a nonfinite form .
\end{enumerate}



\paragraph{Korean Verbs}

We then considered morphemes occurring at least 50 times:

\begin{enumerate}
    \item Root (Valency is not separated in the dataset)
    \item Derivation:
    
    ha (from hada), i (predicative)
    
    \item Honorific -s-
    \item Tense/Aspect: -ess- for past, -essess- for remote past, -keyss- for future
    \item Formality -p-
    \item Mood 1:
    
    -n-
    
    -ni-
    
    -ri-
    
    -deon-
    
    \item `Pragmatic Mood'
    
    -da-
    
    -ra-
    
    Interrogative -ka, -lkka, -nya
    
    -ji
    
    -eo (informal)
    
    -sida
    
    ....
    
    \item Polite -yo
    \item Conjunctive endings
    
    -go
    
    -seo
    
    and others
    
\end{enumerate}

%\ex.\ag. oa di rek a \\
%\textsc{subject.agreement} \textsc{object.agreement} buy \textsc{indicative} \\
%`(he) is buying (it)'  \citep{demuth1992acquisition} \label{ex:oadireka}
%\bg. o pheh el a \\
%\textsc{subject.agreement} cook \textsc{applicative} \textsc{indicative} \\
%`(he) cooks (food) for (him)'  \citep{demuth1992acquisition}
%\label{ex:ophehela}

\begin{tabular}{llllllllll}
1    & 3 & 4     & 5   & 6  & 7 & 8 \\
bara & sy & eots & eum & ni & da  & &  `wished'\\
wish & Honorific & Past & Formal & Indicative & Indicative \\
bara& & gess &  & &  eo & yo  &  `will wish' \\
wish  & &  Assertive && & informal & polite \\
\end{tabular}

%https://en.wiktionary.org/wiki/%EB%B0%94%EB%9D%BC%EB%8B%A4

\section{Results}

\section{Relation to previous accounts}

\subsection{Semantic Scope; Syntactic Structure; Historical Development}

TODO collect citations


In this section, we discuss the linguistic literature on morpheme ordering.

To a significant extent, we do not see the memory--surprisal tradeoff as competing with these.
Rather, it specifies tendencies for ordering that could be satisfied at different levels of description.

The most straightforwardly related account is that of Bybee (1985) Semantic relevance

CARP template in Bantu


Explanations for the orders


A related, though different, account is in terms of semantic scope \cite{rice2000morpheme}.

The scope hypothesis is attractive for some morphemes.
For instance, 


\cite{baker1985the} proposed the Mirror Principle, stating that the order of morphemes reflects ...

This can be related to the scope-based explanation to the extent that syntactic structure and word order reflect scope.

\cite{muysken1981quechua}

\cite{mccarthy2008generalized} phonology

\cite{hyman2003suffix}

\cite{kanu2009suffix} morphotactics, not semantic scope


\subsection{Relevance; Proximity; Iconicity}

The relation  between mutual information and relevance

\subsection{Beyond Agglutionation}

\section{Discussion}




\section{Conclusion}

\bibliography{literature}
\bibliographystyle{natbib}

\end{document}






