\documentclass[11pt,letterpaper]{article}
% \documentclass[man]{apa7}

\usepackage{showlabels}
\usepackage{fullpage}
\usepackage{pslatex}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{url}
\usepackage{rotating}
%\usepackage{natbib}
\usepackage{amssymb}
\usepackage{lingmacros}


%\usepackage{linguex}
%\usepackage{lingmacros}
%\usepackage{CJKutf8}
%\newcommand{\korean}[1]{\begin{CJK}{UTF8}{mj}#1\end{CJK}}

\usepackage{tikz}


\usepackage{colortbl}

%\usepackage{xeCJK}
%\usepackage{natbib}

\bibliographystyle{unsrtnat}

%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
%\usepackage{bm}
\usepackage{graphicx}
%\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{tikz-dependency}
\usepackage{changepage}
\usepackage{longtable}


\usepackage{csquotes}
\usepackage[bibencoding=utf8, style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
%\usepackage[backend=biber, bibencoding=utf8, style=authoryear, citestyle=authoryear]{biblatex} % https://tex.stackexchange.com/questions/402714/biblatex-not-working-with-biber-2-8-error-reading-bib-file-in-ascii-format
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{literature.bib}



\title{Morpheme Ordering across Languages reflects optimization for Memory Efficiency}
%\shorttitle{Morpheme Ordering reflects Memory Efficiency}

\author{Anonymous}
%\affiliation{}

%\leftheader{Anonymous}




\newcommand{\citep}{\parencite}
\newcommand{\citet}{\Textcite}



\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\Prob}[0]{\mathbb{P}}
\newcommand{\Ff}[0]{\mathcal{F}}

\usepackage{multirow}

\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\michael[1]{{\color{red}(#1)}}
\newcommand\mhahn[1]{{\color{red}(#1)}}
\newcommand\becky[1]{{\color{blue}(#1)}}
\definecolor{Pink}{RGB}{255,50,170}
\newcommand{\jd}[1]{\textcolor{Pink}{[jd: #1]}}  
\newcommand{\key}[1]{\textbf{#1}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathop{\mathbb{E}}}



%\usepackage{lingmacros}
%\usepackage{linguex}


\usepackage{amsthm}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}

\newcounter{theorem}
\newcounter{def}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{thm}[theorem]{Theorem}
\newtheorem{definition}[def]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}

%\abstract{
%}

%\keywords{Language universals, sentence processing, information theory}

%\authornote{
%   \addORCIDlink{Michael Hahn}{0000-0003-4828-4834}
%
%  Correspondence concerning this article should be addressed to Michael Hahn, Department of Linguistics, Stanford University Stanford, CA 94305-2150.  E-mail: mhahn2@stanford.edu
%  
%All code and data is freely available at \url{https://github.com/beckymathew/morpheme-ordering}.
%  }

\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thesection}{S\arabic{section}}

\newcommand{\utterance}{\mathcal{U}}
\newcommand{\tree}{\mathcal{T}}



%\usepackage{siunitx}



\usepackage{longtable}



%\frenchspacing
%\def\baselinestretch{0.975}

%\emnlpfinalcopy
%\def\emnlppaperid{496}


\begin{document}


\maketitle

\begin{abstract}
    The order of morphemes in a word shows well-documented tendencies across languages.
Previous work has explained these in terms of notions such as semantic scope and relevance.
A recent theory argues that word and morpheme order in language optimizes a tradeoff between Memory and surprisal, and provided initial evidence from two languages that morpheme order can partly be explained by optimization for this treadeoff.
In this work, we test this idea more extensively using data from four additional agglutinative languages with significant amounts of morphology, in both verb and noun inflection.
We find that the memory-surprisal tradeoff predicts order in most cases with high accuracy.
We also find that it accounts for several, previously documented, universal properties, such as the relative order of number and case suffixes on nouns.
Our work adds to a growing body of work suggesting that many properties of language arise from cognitive principles.
\end{abstract}


%\begin{abstract}
%\end{abstract}


\section{Introduction}

Human language encodes thoughts into linear strings of words, and ultimately sounds.
Across languages, words are composed of morphemes, commonly defined as the smallest meaning-bearing units of language~\citep{bloomfield1926a,katamba2006morphology}.
%In many cases, words can be segmented into a sequence of morphemes, as in
For instance, the English word ``runners'' can be decomposed into three morphemes: the root run- indicating an action, the suffix -er- indicating someone performing an action, and plural -s indicating a group of several referents.
The order of morphemes within a word follows well-documented cross-linguistic tendencies, for instance, morphemes deriving new words (e.g., English -er deriving nouns from verbs) are ordered closer to the root than inflectional morphemes (e.g., English plural -s).
Another well-documented universal applying to languages with richer morphology is Greenbergs' Universal 39:
Plural markers tend to be closer to noun stems than case markers \citep[112]{greenberg1963universals} (see Section~\ref{sec:univ-nouns} below).
In morphologically rich languages, verbs often have a string of two or more affix morphemes attached to a root, and the typological literature has documented universal tendencies, such as a preference for morphemes changing the number of arguments to appear close to the root, while morphemes indicating agreement with subjects appear far away from the root \citep{bybee-morphology-1985}.

Explaining these linguistic universals has been an important subject of study.
Explanations have been stated in terms of the relative scope of different morphemes  \citep{rice2000morpheme}, parallelism between morphology and syntax \citep{givon1971historical,venneman1973explanation,baker1985the}, and the strength of their semantic association between different morphemes \citep{bybee-morphology-1985}.
TODO cite more here.
%One family of explanations holds that those affixes are closest to the root that are most relevant to the root \citep{bybee-morphology-1985}.
%Other explanations suggest that morphemes are ordered based on (TODO CITE).

A recent theory proposes a cognitive explanation for word and morpheme order in language, arguing that ordering universals in language optimize processing effort under memory limitations \citep{Hahn2020modeling}.
Formally, they introduced the notion of a \emph{memory-surprisal tradeoff}: Depending on how many memory resources a comprehender invests, they can achieve different levels of surprisal.
\citet{Hahn2020modeling} argue that order of words and morphemes in language optimizes this tradeoff.
Optimizing the memory-surprisal tradeoff amounts to putting elements together when they predict each other strongly, as measured by mutual information.
In a case study, \citet{Hahn2020modeling} apply this to morpheme ordering in verbs in two languages (Japanese and Sesotho), finding that optimization could partly reproduce morpheme ordering in these languages.
They also suggested that mutual information formalizes previous notions of strength of semantic association between morphemes \citep{bybee-morphology-1985}. %, such as \cite{bybee-morphology-1985}'s notion of \textit{relevance}.






In this work, we examine this theory on a broader basis.
First, we consider data from four additional languages, testing not only whether it accounts for universals of verb affix order documented by \cite{bybee-morphology-1985}, but also extending the scope of the analysis to nouns, testing whether the theory accounts for Greenberg's Universal 39 mentioned above.

We focus on languages with rich morphology where words tend to have multiple morphemes and these are mostly realized separately.
Such languages are referred to as agglutinative \citep{humboldt1836uber,greenberg1960a}.
We obtained data from four languages where substantial amounts of corpus data with detailed morphological annotation are available (Korean, Turkish, Hungarian, Finnish).
These languages have very substantial verbal inflection, and three of these languages (Turkish, Hungarian, Finnish) also have substantial noun inflection.
%Whereas \cite{Hahn2020modeling} only considered verb inflection, we also consider noun inflection across three languages.

% interesting https://www.eva.mpg.de/fileadmin/content_files/staff/haspelmt/pdf/Agglutination.pdf

%Greenberg 1963:  "the expression of number almost always comes between the noun base and the expression of case" (Greenberg 1963:112)

%Scope, Syntax, History
%Another family of explanations holds that morpheme ordering is to be explained diachronically, and that morpheme order reflects the order of independent words in earlier stages of a language.
%- Relevance, Proximity Principle, Iconicity




%Bybee on explanation:
%- regarding the historical explanation (Givon 1971, Vennemann 1973):
%Morphology is not always fossilizes syntax (Bybee and Brewer 1980, Bybee 1985 p. 39--40)


\section{Locality and the Memory--Surprisal Tradeoff}

A long line of work in linguistics has proposed principles of locality to account for word order regularities within and across languages.
An early example is \citet{behaghel1932deutsche}, who argued that elements that are closer together in meaning are closer together in form.
In word order, the Head Adjacency or Proximity principles of \citet{frazier1985syntactic,rijkhoff-word-1986} states that words are close to their syntactic heads, and this generalization has since found strong empirical support from data in many languages~\citep[e.g.][]{hawkins-performance-1994,liu2008dependency, futrell-large-scale-2015-1, liu-dependency-2017}.
In morpheme ordering, \citet{bybee-morphology-1985} argues that morphemes are closer to the root when they are more relevant to it.

Such locality principles have been explained in terms of iconicity in form-meaning correspondence \citep{givon1985iconicity}, and in terms of memory and efficiency in online processing \citep{frazier1985syntactic, gibson1998linguistic, hawkins-efficiency-2003, futrell-noisy-context-2017}.
%Based on considerations of memory limitations, \citet{futrell-noisy-context-2017} derived an information-theoretic locality principle that states that words are close to each other when they are highly predictive of each other (see also \citet{culbertson2020from}).
%This principle of information locality TODO


\citet{Hahn2020modeling} proposed a cognitive principle that aims to unify and formalize these locality principles in the form of a Memory--Surprisal Tradoff.
This is a cognitive account of the order of words and morphemes in human language, based on a formalization of memory efficiency in incremental processing.
The memory-surprisal tradeoff links information-theoretic models of memory limitations with surprisal theory.

Surprisal theory \citep{hale2001probabilistic, levy2008expectation} is a theory of the word-by-word processing difficulty in online processing.
It states that the processing effort on a word $w_t$ in context $w-1 ... w_{t-1}$ is proportional to its surprisal
     \begin{equation}   \label{eq:true-surp}
    \text{Difficulty} \propto -\log P(w_t | w_1\dots w_{t-1}).
\end{equation}
Surprisal as estimated by corpus-based methods or cloze is a successful predictor of reading time on naturalistic text \citep{smith2013effect,goodkind-predictive-2018,frank2019interaction,aurnhammer2019evaluating,wilcox2020predictive}. % (TODO citation based on Cloze).
Surprisal theory is a computational-level theory; it can be justified in terms of different mechanisms, including preactivation and integration~\citep{kuperberg2016we}.
\citet{futrell-noisy-context-2017-1} and \citet{Hahn2020modeling} argue that, due to limitations in human memory, human expectations in reality do not reflect the true context $w_1\dots w_{t-1}$, but some potentially lossy memory representation $m_t$ of the context $w_1\dots w_{t-1}$:
\begin{equation}   \label{eq:lossy-surp}
    \text{Difficulty} \propto -\log P(w_t | m_t).
\end{equation}
\citet{Hahn2020modeling} note that there is a tradeoff between average surprisal and memory capacity:
The more information a listener stores in $m_t$, the lower their surprisal will be on average.
This is because higher precision of memory leads to more precise expectations, which will achieve lower surprisal on average.

More formally, they consider functions $M$ describing how comprehenders update memory representations $m_{t-1}$ when observing a word (or morpheme) $w_t$ to a new memory state $m_t := M(m_{t-1}, w_t)$.
The memory capacity is formalized as the average number of bits required to encode $m_t$, i.e., its entropy:
\begin{equation*}
    \operatorname{H}[m_t] := - \sum_m P(m_t = m) \log_2 P(m_t=m)
\end{equation*}
where $m$ runs over possible memory states.
\citet{Hahn2020modeling} prove that there is a tradeoff between, on the one hand, the average surprisal $S\_M$, obtained by averaging $- \log P(w_t , m_t)$ across the words in a text, and the memory capacity $\operatorname{H}[m_t]$.

\mhahn{at this point illustrate with a figure?}

Based on this notion, \citet{Hahn2020modeling} argue that human language orders elements in such a way that the memory-surprisal tradeoff curve has a particularly steep fall-off, compared to other possible orderings.


\citet{Hahn2020modeling} provide a method for estimating a bound on the memory-surprisal tradeoff from corpus data, and show that orderings achieve efficient tradeoffs when elements that predict each other are close together.
This method is based on the notion of mutual information, which quantifies the amount of statistical association between two random variables.
If $X, Z, Y$ are random variables, then the mutual information of $X$ and $Y$, conditioned on $Z$, is defined to be:
\begin{align}
\label{eq:mi}
    \operatorname{I}[X:Y|Z] &\equiv \sum_{x,y,z} P(x,y,z) \log \frac{P(x,y,z)}{P(x,z)P(y,z)}. % \text{ bits} \\
    %\nonumber
    %&= \operatorname{H}[X,Z] - \operatorname{H}[X,Y,Z] \\
    %\nonumber
    %&= \operatorname{H}[Y,Z] - \operatorname{H}[Y,X,Z].
\end{align}
The key quantity derived from this is the mutual information between elements (such as morphemes) that are some distance $t$, conditioned on the intervening elements:
\begin{equation*}
    I_t \equiv \operatorname{I}[w_t : w_0 | w_1, \dots, w_{t-1}].
\end{equation*}
Based on this notion, \citet{Hahn2020modeling}  prove the following bound on the memory-surprisal tradeoff :
\begin{thm}\label{prop:suboptimal}(Information locality bound, \citet{Hahn2020modeling})
For any positive integer $T$, let $M$ be a memory encoding function such that
\begin{equation}
\label{eq:memory-bound}
\operatorname{H}[m_t] \le \sum_{t=1}^T t I_t.
\end{equation}
Then we have a lower bound on the average surprisal $S_M$ under the memory encoding function $M$:
\begin{equation}
\label{eq:surprisal-bound}
S_M \ge S_\infty + \sum_{t=T+1}^\infty I_t.
\end{equation}
where $S_\infty$ is the average surprisal that would be achieved with perfectly veridical memory representations.
\end{thm}

\mhahn{again, at this point illustrate with a figure?}

A key consequence of this theorem is an information-theoretic notion of locality called Information locality:
Informally, orderings optimize this tradeoff when elements with high mutual information are closer together.
%Previously, a similar notion of information locality was derived by \citet{futrell2020lossy} for a specific family of memory representations $M$.
Information locality has had success as a predictor of word order \citep{futrell2019information}, in particular for universals of the order inside noun phrases \citep{culbertson2020from,hahn-information-theoretic-2018,DBLP:conf/acl/FutrellDS20}.



\citet{Hahn2020modeling} argue that information locality derives a range of locality principles proposed in the linguistic literature.
They provide evidence that it predicts closeness of syntactically related words.
They also provide preliminary evidence that it accounts for some properties of morpheme ordering and formalizes \cite{bybee-morphology-1985}'s idea that morphemes are closer together when they are more relevant to each other, using data of verb inflection in two languages (Japanese and Sesotho).

In this work, we aim to test the Memory-Surprisal Tradeoff as a predictor of morpheme ordering more broadly, using data from more languages and from different parts of speech.



\section{Morpheme Ordering and Ordering Universals}

%\jd{give this section more structure}

In this section, we introduce the phenomena we seek to explain: crosslinguistic tendencies in the order of affix morphemes in nouns (Section~\ref{sec:univ-nouns}) and verbs (Section~\ref{sec:univ-verbs}).

Languages apply inflection to different classes of words, including both open word classes such as nouns, verbs, and adjectives, and closed word classes, in particular pronouns.
In this work, we focus on open word classes, as these have productive paradigms that apply to thousands of words in a language, including words that newly enter the language, whereas pronominal inflection is restricted to a small number of words, often with idiosyncratic and fossilized paradigms inherited from earlier stages of a language.
Among open word classes, inflection commonly applies to verbs, nouns, and adjectives.
When adjectives are inflected, they often pattern with either verbs -- when they are used as predicates -- or nouns -- when they are used as attributes or independent nouns.
We thus focus on nouns and verbs, treating adjectives together with one of the other classes depending on the language when appropriate (with verbs in Korean and Japanese; with nouns in Hungarian, Finnish, Turkish. TODO Sesotho?).

\subsection{Universals of Noun Affixes}\label{sec:univ-nouns}
Nouns very commonly mark number and case morphologically.
%Number marking typically distinguishes singular and plural (person -- person-s).
%Case marking 
In some languages, possession is also marked on the noun (CITE).
Figure~\ref{fig:noun-inflection} shows fully inflected nouns in three languages from our sample, with endings for number, case, and possessor.
Number and case marking are the subject of a well-documented universal, namely \citep{greenberg1963universals}'s Universal 39:

\begin{adjustwidth}{6em}{6em}
\textsc{Greenberg's Universal 39}:
``the expression of number almost always comes between the noun base and the expression of case'' (Greenberg 1963:112).
\end{adjustwidth}
%It states that number affixes are closer to the stem than case affixes, if they appear on the same side.

This universal is supported by the example in Figure~\ref{fig:noun-inflection}.

%\ex.\ag. mi  rare mash yoo \\
%%Stem (3) (5) (8) \\
%see  \textsc{passive}  \textsc{politeness}  \textsc{future} \\
%`will be seen'
%\bg. mi taku nakat ta \\
%%Stem (6) (7) (8) \\
%see \textsc{desiderative} \textsc{negation} \textsc{past} \\
%`did not wish to see'


\begin{figure}
\begin{tabular}{lllllllll}
\textit{juttu-i-hi-si} \\
Stem-Plural-Illative-2SgPoss \\
`into your stories' (Finnish)
\end{tabular}
\begin{tabular}{lllllllll}
	\textit{{\'e}rv-ei-k-et} \\
Stem-Plural-3rdPlurPoss-Accusative \\
`their arguments' (Hungarian)
\end{tabular}
\begin{tabular}{lllllllll}
\textit{hareket-ler-im-den} \\
Stem-Plural-1sgPoss-Ablative\\
`from my actions' (Turkish) \\
\end{tabular}

\mhahn{TODO make this a nice illustration of noun inflection appropriate for the audience}

\caption{Noun inflection}\label{fig:noun-inflection}
\end{figure}



 \begin{figure}
     Valence:
\begin{tabular}{lllll}
	a) & {\c c}{\i}ld{\i}r-{\i}r-{\i}m \\
& go.crazy-Causative-aorist-1sg \\
& `I go crazy' (Turkish)\\
b) & {\c c}{\i}ld{\i}r-t-{\i}r-{\i}m \\
& go.crazy-Causative-aorist-1sg \\
& `I make (someone) go crazy' (Turkish) \\
\end{tabular}     
Voice:
\begin{tabular}{llllll}
a) & mi-mash-yoo \\
& see-politeness-future \\
& `will see' (Japanese) \\
b) & mi-rare-mash-yoo \\
& see-passive-politeness-future \\
&`will be seen' (Japanese)
\end{tabular}

Mood:
\begin{tabular}{llll}
a) & ismer-nek \\
& know-3pl.def \\
& `they know' (Hungarian) \\
b) & ismer-het-nek \\
& know-potential-3pl.def \\
& `they can know' (Hungarian)
\end{tabular}
Tense:
\begin{tabular}{llllll}
a) & isseo-yo \\ 
& be-polite \\
& `is/are' (informal polite, Korean) \\ 
b) & isseo-sseo-yo \\
 & be-past-polite \\
 & `was/were' (informal, polite, Korean)
 \end{tabular}
 Agreement:
\begin{tabular}{lllll}
a) & a{\c c}ar-{\i}m \\
b)& a{\c c}ar-s{\i}n \\
c) & a{\c c}ar-{\i}z \\
d) &a{\c c}ar-s{\i}n{\i}z \\
& stem-person \\
& `I/you (sgd)/we/you (pl) open'
\end{tabular}

\mhahn{TODO make this a nice illustration of verb inflection appropriate for the audience}
\caption{Examples of verb inflection}\label{tab:examples-verbs}
\end{figure}
 

\begin{figure}

  \begin{tikzpicture}[%
% common options for blocks:
block/.style = {draw, fill=blue!30, align=center, anchor=west,
            minimum height=0.65cm, inner sep=0},
% common options for the circles:
ball/.style = {circle, draw, align=center, anchor=north, inner sep=0}]
%\node[rectangle,text width=1.2cm,anchor=base] (A0) at (1,-0.3) {Real};


%\node[rectangle,text width=1.2cm,anchor=base] (A1) at (1,-1.0) {1Derivation};
\node[rectangle,text width=1.2cm,anchor=base, color=red] (A2) at (1,-1.5) {Valence};
\node[rectangle,text width=1.2cm,anchor=base, color=green] (A3) at (1,-2.0) {Voice};
\node[rectangle,text width=1.2cm,anchor=base,color=blue] (A4) at (1,-3) {TAM};
%\node[rectangle,text width=1.2cm,anchor=base,color=blue] (A4) at (1,-3.5) {Tense};
%\node[rectangle,text width=1.2cm,anchor=base,color=blue] (A4) at (1,-4) {Mood};
\node[rectangle,text width=1.2cm,anchor=base,color=orange] (A4) at (1,-5) {Agreement};

% Turkish
\node[rectangle,text width=0.9cm,anchor=base] (B0) at (3,-0.3) {\textbf{Turkish}};
\node[rectangle,text width=1.2cm,anchor=base,color=red] (B1) at (3,-1.5) {1Causative};
\node[rectangle,text width=1.2cm,anchor=base,color=green] (B2) at (3,-2) {2Passive};
\node[rectangle,text width=1.2cm,anchor=base] (B3) at (3,-2.5) {3Negation};
\node[rectangle,text width=1.2cm,anchor=base,color=blue] (B4) at (3,-3) {4Potential};
\node[rectangle,text width=1.2cm,anchor=base,color=blue] (B4) at (3,-3.5) {5TAM1};
\node[rectangle,text width=1.2cm,anchor=base,color=orange] (B4) at (3,-4) {6.3rdPl};
\node[rectangle,text width=1.2cm,anchor=base,color=blue] (B4) at (3,-4.5) {7.TAM2};
\node[rectangle,text width=1.2cm,anchor=base,color=orange] (B4) at (3,-5) {8.Agreement};

% Hungarian
\node[rectangle,text width=0.9cm,anchor=base] (B0) at (5,-0.3) {\textbf{Hungarian}};
\node[rectangle,text width=1.2cm,anchor=base,color=green] (B2) at (5,-2) {1Passive};
\node[rectangle,text width=1.2cm,anchor=base,color=blue] (B4) at (5,-3) {2Potential};
\node[rectangle,text width=1.2cm,anchor=base,color=blue] (B4) at (5,-3.5) {3Tense};
\node[rectangle,text width=1.2cm,anchor=base,color=orange] (B4) at (5,-5) {4Agreement};


% Finnish
\node[rectangle,text width=0.9cm,anchor=base] (B0) at (7,-0.3) {\textbf{Finnish}};
\node[rectangle,text width=1.2cm,anchor=base,color=green] (B2) at (7,-2) {1Passive};
\node[rectangle,text width=1.2cm,anchor=base,color=blue] (B4) at (7,-3.5) {2TAM};
\node[rectangle,text width=1.2cm,anchor=base,color=orange] (B4) at (7,-5) {3Agreement};

% Sesotho Prefixes
\node[rectangle,text width=0.9cm,anchor=base] (B0) at (9,-0.3) {\textbf{SesP}};
\node[rectangle,text width=1.2cm,anchor=base,color=green] (B2) at (9,-2) {1Ob.ag.};
\node[rectangle,text width=1.2cm,anchor=base] (B4) at (9,-2.5) {2Polarity};
\node[rectangle,text width=1.2cm,anchor=base,color=blue] (B4) at (9,-3) {3Tense};
\node[rectangle,text width=1.2cm,anchor=base,color=orange] (B4) at (9,-5) {4Agreement};
\node[rectangle,text width=1.2cm,anchor=base,color=purple] (B4) at (9,-5.5) {5Int./Rel.};

% Sesotho Suffixes
\node[rectangle,text width=0.9cm,anchor=base] (B0) at (11,-0.3) {\textbf{SesS}};
\node[rectangle,text width=1.2cm,anchor=base] (B2) at (11,-1) {1Reversive};
\node[rectangle,text width=1.2cm,anchor=base,color=red] (B2) at (11,-1.5) {2Valence};
\node[rectangle,text width=1.2cm,anchor=base,color=green] (B2) at (11,-2) {3Passive};
\node[rectangle,text width=1.2cm,anchor=base,color=blue] (B4) at (11,-3) {4Mood};



% Japanese
\node[rectangle,text width=0.9cm,anchor=base] (B0) at (13,-0.3) {\textbf{Japanese}};
\node[rectangle,text width=1.2cm,anchor=base] (B2) at (13,-1) {1Derivation};
\node[rectangle,text width=1.2cm,anchor=base,color=red] (B2) at (13,-1.5) {2Causative};
\node[rectangle,text width=1.2cm,anchor=base,color=green] (B2) at (13,-2) {3Pass./Pot.};
\node[rectangle,text width=1.2cm,anchor=base] (B4) at (13,-2.5) {4Politeness};
\node[rectangle,text width=1.2cm,anchor=base,color=blue] (B4) at (13,-3) {5Desiderative};
\node[rectangle,text width=1.2cm,anchor=base] (B4) at (13,-3.5) {6Negation};
\node[rectangle,text width=1.2cm,anchor=base,color=blue] (B4) at (13,-4) {7TAM/Emb.};

% Korean
\node[rectangle,text width=0.9cm,anchor=base] (B0) at (15,-0.3) {\textbf{Korean}};
\node[rectangle,text width=1.2cm,anchor=base] (B2) at (15,-1.0) {Derivation};
\node[rectangle,text width=1.2cm,anchor=base] (B4) at (15,-2.5) {Honorific};
\node[rectangle,text width=1.2cm,anchor=base,color=blue] (B4) at (15,-3) {Tense};
\node[rectangle,text width=1.2cm,anchor=base] (B4) at (15,-3.5) {Formality};
\node[rectangle,text width=1.5cm,anchor=base,color=blue] (B4) at (15,-4.0) {Mood I};
\node[rectangle,text width=1.5cm,anchor=base,color=blue] (B4) at (15,-4.5) {Mood II};
\node[rectangle,text width=1.2cm,anchor=base] (B4) at (15,-5) {Politeness};
\node[rectangle,text width=1.2cm,anchor=base,color=purple] (B4) at (15,-5.5) {Embedding};


%\draw[->] (A1.east) to (B1.west);
%\draw[->] (A2.east) to (B2.west);
%\draw[->] (A3.east) to (B3.west);
%\draw[->] (A4.east) to (B4.west);
\end{tikzpicture}

%    \centering
%\begin{tabular}{l||l|l|l|l|l|l|llll}
%                    & Turkish & Hungarian & Finnish  & Sesotho Pref.     & Ses Suff. & Japanese & Korean\\ \hline\hline
%Derivation          &  &         &          &               & Reversive & suru    & ha,i\\ \hline
%Valence             &  Causative &         &           & Object marker & Valence & Causative\\ \hline
%Voice               & Passive & Passive    & Passive     &               & Passive & Passive\\ \hline
%TAM, Pol.           & Negation  &   Potential  &   Tense/Asp &    Negation &  Politeness &      Potential        & Honorific \\
%                    & Potential & Tense        &    Mood     &     Tense   &    Mood     &   Honorific  &    Tense       \\
%                    &   TAM1    &          &                &         &                  & Tense/Aspect & Formality \\
%                    & lar       &          &           &  & & Negation & Mood I\\
%                    & TAM2         &           &               &          &       &      &  Mood II \\ \hline
%Agreement           & Agreement & Agreement & Agreement & Agreement \\ \hline
%\textit{Other}               & Formality          & Clitic    &              & Int/Rel &      &        & Politeness \\
%                    &           &     &              &  &          &    & Conj \\
%\end{tabular}

\mhahn{TODO make this visually more appealing}

    \caption{Morpheme order across the six languages in our sample, matched with four universal categories (left).}
    \label{tab:table-orders}
\end{figure}

\subsection{Universals of Verb Affixes}\label{sec:univ-verbs}
Verbs commonly mark a larger number of inflection categories.
Figure~\ref{tab:table-orders} summarises the affixes in the verbal morphologies of the six morphologically rich languages considered here.
For each language in the sample, we determined a templatic sequence of slots into which morphemes are ordered (see SI Section X for details on how we arrived at these slots).

Verbal affixes are typically grouped into derivational and inflectional affixes.
Derivational affixes derive new verb stems (e.g., 'do' $\rightarrow$ undo), whereas inflectional affixes derive inflected verb forms from verb stems (e.g., `do' $\rightarrow$ `does').
Examples of derivational suffixes are Japanese -su- and Korean -ha-, which derive verbs from non-verbal stems \citep{hasegawa2014japanese, yeon2010korean}.
Another example is the Reversive suffix in Sesotho (corresponding to `un-' in English `do' $\rightarrow$ `undo', \citep{doke1967textbook}).
Derivational affixes generally appear closer to the root than inflectional affixes.

\textit{Valence} affixes (red in Figure~\ref{tab:table-orders}, Example X in Figure~\ref{tab:examples-verbs}) change the number of arguments. A very common type of valence affix is a causative, which adds an argument indicating who causes an event or state to occur \citep{wals-111}.
%In Turkish, -t- marks causatives (\citep{schaaik2020turkish}, Figure~\ref{tab:examples-verbs}).
\textit{Voice} (green in Figure~\ref{tab:table-orders}) describes the distinction between active and passive (CITE). An example is the Japanese passive suffix -\textit{(r)are}- (Figure~\ref{tab:examples-verbs}).
\textit{Tense-Aspect-Mood (TAM)} (blue in Figure~\ref{tab:table-orders}) comprises three types of categories.
\textit{Aspect} describes how an event unfolds over time \citep{comrie1976aspect,dahl1985tense,binnick1991time}.
An example of aspect marking is the English progressive \textit{to be} ...-\textit{ing}, which indicates that an action is currently ongoing (CITE).
\textit{Tense} describes where an event is located in time (e.g. past or future, CITE).
Aspect and tense categories are often fused in morphology \citep{binnick2012the}, and this also applies to the languages in our sample.
Examples of tense and aspect marking are shown in Figure~\ref{tab:examples-verbs}.
\textit{Mood} describes a relation between an event and the speaker, including an assessment of the event's reality \cite{palmer1986mood,portner2018mood}.
In analytical languages such as English, mood distinctions are often indicated using verbs such as \textit{might} or adverbs such as \textit{possibly}.
A common mood category is the Potential mood, which indicates possibility (Turkish, Hungarian).
Figure~\ref{tab:examples-verbs} shows how it is marked morphologically with -het- in Hungarian \citep{rounds2001hungarian}.
In many languages, mood marking is at least partically fused with tense and aspect; for instance, the Japanese suffix -\textit{yoo} can mark future tense and the hortative mood (`let's ...', \citep{hasegawa2014japanese}).
Tense, aspect, mood are often together referred to as TAM (Tense-Aspect-Mood, \citep{bybee1994the, wals-69}).
Some languages have a single affix slot that accommodates a fused morpheme indicating TAM.
For instance, Finnish marks both tense (present and past) and mood (indicative, conditional, and potential) categories with a single morpheme in the slot labeled TAM.
Other languages have multiple slots, for instance, Turkish has (TODO explain TAM1, TAM2).
\textit{Subject agreement} (orange in Figure~\ref{tab:table-orders}) mark categories of the subject, most often its person and number, sometimes also other categories such as its gender.
An example is English third-person -s.
% re agreement for references: http://epubs.surrey.ac.uk/1063/1/fulltext.pdf
In our sample, Turkish, Hungarian, Finnish, and Sesotho have a subject agreement slot (Figure~\ref{tab:table-orders}); Turkish also has a special slot occupied only by the third-person plural marker \textit{-lar-} (indicated as 3rdPl in ~\ref{tab:table-orders}).

The ordering of morphemes encoding these categories shows universal tendencies \citep{bybee-morphology-1985}, which we summarize as follows:


\begin{adjustwidth}{6em}{6em}
\textsc{Verb Affix Ordering} \citep{bybee-morphology-1985}:
Verb affixes are ordered as follows, outward from the verb stem:

\begin{tabular}{llllllllllllllllllllllllll}
verb stem & valence & voice & TAM & subject agreement
\end{tabular}
\end{adjustwidth}

%Based on data from several dozens of languages, \citep{bybee-morphology-1985} proposes the following ordering of these common verb affixes:
%\begin{quote}
%\begin{tabular}{llllllllllllllllllllllllll}
%verb stem & valence & voice & aspect & tense& mood & subject agreement
%\end{tabular}
%\end{quote}

Figure~\ref{tab:table-orders} shows that the languages in our sample largely support this universal, with the exception of the order of the special third-plural suffix slot in Turkish, which intervenes between two TAM slots.
\citet{bybee-morphology-1985} also provides evidence for ordering preferences within aspect, tense, and mood; however, we do not distinguish between them as these are frequently fused in languages.


While these are particularly common types of affixes, there are further types, some of which occur in the six languages of our sample.
While agreement is most commonly established with the subject, \textit{agreement with the object} is found in Sesotho \citep{doke1967textbook} (in person and noun class) and in Hungarian \citep{rounds2001hungarian} (in definiteness); it is fused with subject agreement in the latter case.
\textit{Polarity} refers to the opposition between affirmative (e.g. `she arrived') and negative  (e.g., `she did not arrive') statements \citep{wals-112}.
\textit{Evidentiality} indicates on what evidence a speaker bases an assertion \citep{aikhenvald2003evidentiality}.
\textit{Formality}, \text{honorifics}, and \text{politeness} are categories that index social relations between the speaker, the addressee, and the topic of the conversation (CITE).
In our sample, these are prominent in Korean and Japanese.
The Japanese politeness marker -masu- and the Korean formality (-p) and politeness (-yo) suffixes index the social relation between the speaker and the addressee \citep{hasegawa2014japanese, yeon2010korean}; the Korean honorific suffix -si- indexes the social relation between the speaker and the topic of the conversation \citep{yeon2010korean}.

Furthermore, verb forms can have affixes indicating the syntactic position of the verb within a sentence, in particular, affixes marking infinitives or other nonfinite forms.
We use the term \textit{Embedding} (TODO) for this type of morpheme.

\jd{add segue to next section}

%We focus on inflection, except in those cases where derivational affixes are clearly marked in available data.
%Inflectional suffixes are generally outside of derivational affixes

% somewhere explain the choice of languages
% - rich agglutinative.
% - must have data with suitable morphological analysis.
% - Among the languages, Hungarian and Finnish are genetically related (X millenia). There is also some evidence for genetic relations beyond these (Japanese, Korean, Turkish), but such relations would have to be quite ancient (x millenia).
% The morphemes found in these languages as considered here are generally not cognate.



\section{Methods}

\subsection{Data} % TODO: Becky
% why agglutinative?


We obtained data from four morphologically rich languages.
We focused on languages in which verbs and nouns often have more than two morphemes per words, as that allows us to test predictions about the relative ordering of different morphemes.
Furthermore, we focused on languages in which the morphemes within a word have clearly delimited boundaries, providing unambiguous information about the ordering of morphemes.
Languages with these properties are referred to as agglutinative.
Mutual information depends on the frequencies at which different morphological forms are used. We thus selected agglutinative languages for which large-scale text corpora with morphological annotation was available.
The language sample includes Universal Dependencies (UD) corpora for Korean \citep{chun2018building}, Turkish \citep{turkish-imst}, Hungarian \citep{hungarian-szeged}, and Finnish \citep{UDFinnish-TDT}.
In addition, we also reanalyze the data from \citet{Hahn2020modeling}, covering UD data for Japanese \citep{asahara2018universal} and the Child Language Data Exchange System (CHILDES) Sesotho corpus \citep{demuth1992acquisition} in a way consistent with our analysis of the other four languages.


\mhahn{TODO corpus sizes.}


For nouns, we focused on Turkish, Hungarian, and Finnish, as nouns in these languages often have more than one affix.
We used all six languages for verbs.

For each language, we selected nouns and verbs based on the part-of-speech annotation in each corpus.
We then identified which morphemes each extracted word was composed of, on the basis of the annotation provided in the corpora (see SI Appendix, Section X for details).



\subsection{Estimating memory-surprisal tradeoffs}
%\jd{change title to something more informative, eg "Estimating memory-surprisal tradeoffs"?}

For estimating memory-surprisal tradeoffs, we modeled words as strings of morphemes, following \citet{Hahn2020modeling}.
For instance, we represent \textit{juttu-i-hi-si} `into your stories' (Figure REF) as juttu-\textsc{Plural}-\textsc{Illative}-\textsc{2sgPoss}.
(See SI Appendix Section X for experiments taking morphophonological interactions between morphemes into account.)

For each language, we parameterize possible morpheme orderings through the $N!$ possible orderings of the $N$ slots.
Applying any such ordering to the forms extracted from the corpus results in a set of counterfactual forms with some associated memory-surprisal tradeoff curve; in particular, applying the real orderings (Figure REF) results in the actual morpheme sequences as found in the corpus.
Following \citet{Hahn2020modeling}, we quantify the efficiency of a tradeoff curve using its Area under the Curve (AUC). Lower values of AUC indicate a steeper decay of mutual information, and thus a more efficient tradeoff.

We compare the real orderings (\textit{real}) to three different kinds of alternative orderings: randomized morpheme orderings (\textit{random}), the reversed real orderings (\textit{reverse}), and morpheme orderings that are optimized to minimize AUC under the tradeoff curve (\textit{optimized}). 

We estimated memory-surprisal tradeoffs and computationally optimized orderings for the AUC under the tradeoff curve using the method of \citet{Hahn2020modeling}.



%\subsection{Nouns}
%\mhahn{Here is an idea for how we might structure this section: We can describe the systems in the individual languages in prose and in broad strokes here, highlighting what's common and what's different across languages. For the nouns, it's interesting to point out that the relative position of Case and Possessor differs between Finnish and the rest, whereas the position of Number is stable. For the verbs, the discussion could refer to Table~\ref{tab:table-orders}. I've collapsed all the Tense/Aspect/Modality/Negation morphemes into one big category ``TAM, Polarity'' since aligning the morphemes in each language with those individual categories seems very hard, and in any case there are differences between the languages. Talking about this in broad strokes, again highlighting parallels and differences, should be the right level of detail here. You can draw on Table S1 and Tables S5, S12, S13 in the appendix for my revised verb morpheme segmentation in Korean, Turkish, and Hungarian. We can then leave precise and detailed discussion of each of the languages, including detailed references to the literature for these individual languages, to the appendix. The \texttt{enumerate} blocks could go into the appendix, with some description of what each morpheme does and references to reference grammars (I can do this).}
%We're interested in investigating languages where nouns are inflected with multiple suffixes, because that allows us to more extensively measure the relationship between mutual information and mutual information. As such, we only investigated our hypothesis on Turkish, Hungarian, and Finnish nouns. All of these languages inflect nouns for number, possessor person, possessor number, and case, in that order. Finnish is the exception, where we include derivation (verbs that are nominalized, for example), and where the possessor person and number appears after the case suffix in the natural ordering. 
%In contrast, the number suffix, which is generally only marked for the plural, has a stable position close to the root in all three languages. Number has high mutual information with the noun root, because certain nouns have a tendency to appear only in the plural or only in the singular. As such, number must come close to the root.
%\subsection{Verbs}
%For verbs, we investigated Finnish, Hungarian, Japanese, Korean, Sesotho, and Turkish inflectional suffixes. Broadly speaking, all of the languages have a real ordering of valence, voice, negation, tense/aspect/mood (TAM), person and number, and formality, where applicable. \becky{TODO: Check that this is correct and discuss some specifics.} 
%\becky{Which language provides data for which kind of suffix?}
%Following \citet{Hahn2020modeling}, we compare the efficiency each natural language's morpheme orderings (\textit{real}) to three different kinds of baselines: randomized morpheme orderings (\textit{random}), the reversed natural orderings (\textit{reverse}), and morpheme orderings that are optimized to maximize MI (\textit{optimized}). 


%https://en.wiktionary.org/wiki/%EB%B0%94%EB%9D%BC%EB%8B%A4

\section{Results}
Figures \ref{fig:auc_verbs} and \ref{fig:auc_nouns} show area under the curve (AUC) plots for random orders as compared to the real, natural order of morphemes for verbs and nouns, respectively.
%Here, AUC refers to the area under the curve for the memory-surprisal tradeoff for each order, where the lower the AUC is, the more efficient that memory-surprisal tradeoff is.
Across languages, and for both nouns and verbs, real orderings (TODO color) have lower AUC than most random baseline orderings (TODO stats), close to the computationally optimized orderings (TODO color) meaning that they enable more efficient memory-surprisal tradeoffs than most of the $N!$ possible orderings. 

Second, we evaluated to what extent real and optimized orderings agree in the orderings that they define, compared to the agreement between real and random orderings.
Table \ref{tab:optimized_acc} shows the agreement of optimized and random baseline orderings with real orderings.
We defined three measures of agreement:
\textit{Pairs} counts what fraction of all pairs of affixes within a single word from the corpus are ordered in the same relative order under both orderings.
\textit{Full} counts what fraction of words from the corpus is ordered entirely identically under both orderings.
Finally, \textit{Full (Types)} is analogous to \textit{Full} but only counts words that appear multiple times only once, in order to control for the possible effect of individual highly frequent forms.


For nouns, the agreement between real and optimized orderings is perfect, far above the agreement with baseline grammars (TODO stats. how many baseline grammars are outperformed by optimized?).
For verbs, agreement between real and optimized orderings is above 90\% for Hungarian, Turkish, Korean, Japanese, and Sesotho prefixes (TODO stats).
For Sesotho suffixes, agreement is lower, though still better than X\% of baselines.
In Finnish, agreement between real and optimzed orderings is similar to agreement between real and baseline orderings (see below for discussion).

Figures \ref{fig:real_and_optimized_nouns} and \ref{fig:real_and_optimized_verbs} directly compare real and optimized orderings for nouns and verbs respectively.
The real and optimized noun orders show a perfect match.
In particular, Greenberg's Universal (TODO) is recovered by all optimized orderings.
The real and verb optimized orders show some disagreement in each language.
%Divergences in the real and optimized verbal morpheme orders will be analyzed more in the Discussion section. 

%\becky{Refer to S3 to introduce this paragraph}
Figure \ref{fig:optimized_and_universal_orders} compares the universal verb morpheme ordering to the optimized ordering for each language.
Hungarian, Japanese, Korean, and Sesotho (suffixes and prefixes) match the universal ordering perfectly for the morphemes occurring in each of these languages. 
Turkish matches the universal order except for the 3rd person plural agreement marker ``-lar-'', which is separated from the other agreement morphemes in both real and optimized orderings.
For Finnish, the optimized order is the same as the universal order except for the fact that the agreement marker comes closest to the root instead of furthest, which accounts for the low quantitative agreement observed in Table~\ref{tab:optimized_acc}.
This can be traced to a peculiarity of the Finnish passive, which uses a single agreement marker that is only used for passives, and which does not distinguish between person and number categories (referred to as impersonal by \citep[Section 69]{karlsson1999finnish}).
This means that the agreement marker has substantial mutual information with the voice morpheme, so that optimized orderings prefer agreement and voice morphemes to be adjacent.
On active forms -- which, unlike the passive, are fully inflected for person and number -- optimized orderings place the agreement slot after the TAM slot, in agreement with the real ordering.

%Taking a step back and looking at all of these languages at a whole, we see that the languages' optimized orders tend to closely mirror Bybee's universal ordering, which also means that these languages also have a high level of agreement between them on the relative ordering of verbal morphemes. As such, it's possible to predict Bybee's universal ordering from looking at commonalities between optimized orders of natural languages. 


%\becky{conclude}

\begin{figure}
    \centering
    \begin{tabular}{cccccc}
    \textsc{Finnish} & \textsc{Turkish} & \textsc{Hungarian} \\
        \includegraphics[width=0.3\textwidth]{figures/finnish_verbs/suffixes-byMorphemes-auc-hist-heldout-Coarse-FineSurprisal-optimized.pdf}
        &
    \includegraphics[width=0.3\textwidth]{figures/turkish_verbs/suffixes-byMorphemes-auc-hist-heldout-Coarse-FineSurprisal-optimized.pdf}
    &
    \includegraphics[width=0.3\textwidth]{figures/hungarian_verbs/suffixes-byMorphemes-auc-hist-heldout-Coarse-FineSurprisal-optimized.pdf}
    \\
    \textsc{Korean} & \textsc{Japanese} & \textsc{Sesotho Prefixes} \\
    \includegraphics[width=0.3\textwidth]{figures/korean/suffixes-byMorphemes-auc-hist-heldout-Coarse-FineSurprisal-optimized.pdf}
    &
        \includegraphics[width=0.3\textwidth]{figures/japanese/suffixes-byMorphemes-auc-hist-heldout-Coarse-FineSurprisal-optimized.pdf}
        &
            \includegraphics[width=0.3\textwidth]{figures/sesotho_prefixes/suffixes-byMorphemes-auc-hist-heldout-Coarse-FineSurprisal-optimized.pdf}
            \\
            \textsc{Sesotho Suffixes} \\
            \includegraphics[width=0.3\textwidth]{figures/sesotho_suffixes/suffixes-byMorphemes-auc-hist-heldout-Coarse-FineSurprisal-optimized.pdf}
    \end{tabular}

    
    \caption{AUC Histograms for Verb Affixes.}
    \label{fig:auc_verbs}
\end{figure}


\begin{figure}
\begin{tabular}{ccc}
\textsc{Finnish} & \textsc{Turkish} & \textsc{Hungarian} \\
    \includegraphics[width=0.3\textwidth]{figures/finnish_nouns/suffixes-byMorphemes-auc-hist-heldout-Coarse-FineSurprisal-optimized.pdf}
    &
    \includegraphics[width=0.3\textwidth]{figures/turkish_nouns/suffixes-byMorphemes-auc-hist-heldout-Coarse-FineSurprisal-optimized.pdf}
    &
    \includegraphics[width=0.3\textwidth]{figures/hungarian_nouns/suffixes-byMorphemes-auc-hist-heldout-Coarse-FineSurprisal-optimized.pdf}
    \end{tabular}
    \caption{AUC Histograms for Noun Suffixes.}
    \label{fig:auc_nouns}
\end{figure}

\begin{table}[]
    \centering
    \begin{tabular}{l|l|ll|lllllllll}
    &     &    \multicolumn{2}{c|}{Pairs} & \multicolumn{2}{c|}{Full} & \multicolumn{2}{c}{Full (Types)} \\
     &   &     Optim. & Baseline & Optim. & Baseline & Optim. & Baseline \\ \hline
Nouns & finnish  & 1.0 (0.0) & 0.42 (0.31) & 1.0 (0.0) & 0.37 (0.32) & 1.0 (0.0) & 0.38 (0.29) \\
 & turkish  & 1.0 (0.0) & 0.56 (0.36) & 1.0 (0.0) & 0.5 (0.37) & 1.0 (0.0) & 0.5 (0.36) \\
 & hungarian  & 1.0 (0.0) & 0.48 (0.22) & 1.0 (0.0) & 0.33 (0.27) & 1.0 (0.0) & 0.33 (0.27) \\
 \hline
Verbs & finnish verbs & 0.4 (0.3) & 0.4 (0.37) & 0.38 (0.31) & 0.39 (0.38) & 0.36 (0.32) & 0.38 (0.37) \\
 & hungarian  & 0.95 (0.09) & 0.52 (0.37) & 0.95 (0.0) & 0.5 (0.4) & 0.94 (0.0) & 0.5 (0.39) \\
 & turkish  & 0.96 (0.09) & 0.46 (0.14) & 0.95 (0.0) & 0.37 (0.15) & 0.93 (0.07) & 0.36 (0.14) \\
 & korean & 0.97 (0.1) & 0.56 (0.33) & 0.97 (0.0) & 0.53 (0.33) & 0.95 (0.0) & 0.53 (0.36) \\
 & japanese & 0.94 (0.08) & 0.48 (0.24) & 0.93 (0.0) & 0.39 (0.24) & 0.9 (0.1) & 0.39 (0.24) \\
 & sesotho p. & 0.99 (0.0) & 0.5 (0.48) & 0.99 (0.0) & 0.5 (0.48) & 0.98 (0.0) & 0.51 (0.46) \\
 & sesotho s. & 0.62 (0.0) & 0.48 (0.2) & 0.54 (0.0) & 0.45 (0.19) & 0.52 (0.0) & 0.44 (0.19) \\
    \end{tabular}
    \caption{Accuracies in predicting morpheme ordering. `Optim.' indicates grammars optimized for AUC, `'Baseline' indicates randomly constructed weights.
    In each cell, we provide the mean and the stgandard deviation over orderings. `Pairs' indicates the fraction of pairs of morphemes occurring together in the same word that are ordered correctly. `Full' indicates the fraction of verb forms in the corpus that are ordered fully correctly. `Full (Types)' counts forms that occur multiple times only once, it thus down-weights the role of frequent forms.}
    \label{tab:optimized_acc}
\end{table}



\begin{figure}[]
\begin{tabular}{cccccccc}
\textsc{Turkish} & \textsc{Hungarian} & \textsc{Finnish} \\
\begin{minipage}{.3\textwidth}
  \include{figures/turkish_nouns/comparison}
    \end{minipage}
  &
  \begin{minipage}{.3\textwidth}
  \include{figures/hungarian_nouns/comparison}
    \end{minipage}
  &
  \begin{minipage}{.3\textwidth}
  \include{figures/finnish_nouns/comparison}
  \end{minipage}
  \end{tabular}
  
    \caption{Real and optimized ordering (nouns)}
    \label{fig:real_and_optimized_nouns}
\end{figure}


\begin{figure}[]

\begin{tabular}{cccccccc}
\textsc{Turkish} & \textsc{Hungarian} & \textsc{Finnish} \\
\begin{minipage}{.3\textwidth}
  \include{figures/turkish_verbs/comparison}
  \end{minipage}
  &
  \begin{minipage}{.3\textwidth}
  \include{figures/hungarian_verbs/comparison}
  \end{minipage}
  &
    \begin{minipage}{.3\textwidth}
  \include{figures/finnish_verbs/comparison}
  \end{minipage}
  \\
  \textsc{Korean}  & \textsc{Japanese} & \textsc{Sesotho Prefixes} \\
      \begin{minipage}{.3\textwidth}
  \include{figures/korean/comparison}
  \end{minipage}
  &
  \begin{minipage}{.3\textwidth}
  \include{figures/japanese/comparison}
  \end{minipage}
  &
  \begin{minipage}{.3\textwidth}
  \include{figures/sesotho_prefixes/comparison}
  \end{minipage} \\
  \textsc{Sesotho Suffixes} \\
  \begin{minipage}{.3\textwidth}
  \include{figures/sesotho_suffixes/comparison}
  \end{minipage}
  \end{tabular}
  
  
    \caption{Real and optimized ordering (verbs)}
    \label{fig:real_and_optimized_verbs}
\end{figure}



\begin{figure}
\begin{tabular}{ccccccc}
\textsc{Finnish} & \textsc{Hungarian} & \textsc{Turkish} \\
\begin{minipage}{.3\textwidth}
  \include{figures/finnish_verbs/comparison-opt-uni}
    \end{minipage}
    &
    \begin{minipage}{.3\textwidth}
  \include{figures/hungarian_verbs/comparison-opt-uni}
    \end{minipage}
        &
    \begin{minipage}{.3\textwidth}
  \include{figures/turkish_verbs/comparison-opt-uni}
    \end{minipage} 
    \\
    \textsc{Korean} & \textsc{Japanese} & \textsc{Sesotho Prefixes} \\
        \begin{minipage}{.3\textwidth}
  \include{figures/korean/comparison-opt-uni}
    \end{minipage}
    &
    \begin{minipage}{.3\textwidth}
  \include{figures/japanese/comparison-opt-uni}
    \end{minipage}
        &
    \begin{minipage}{.3\textwidth}
  \include{figures/sesotho_prefixes/comparison-opt-uni}
    \end{minipage}
    \\
    \textsc{Sesotho Suffixes} \\
        \begin{minipage}{.3\textwidth}
  \include{figures/sesotho_suffixes/comparison-opt-uni}
    \end{minipage}
\end{tabular}
    \caption{Comparing optimized orders with the universal order described by \citep{bybee-morphology-1985}.  Note that, for Sesotho prefixes, inversion of order relative to the universal order is expected, as the universal order indicates distance from the root, not absolute position.}
    \label{fig:optimized_and_universal_orders}
\end{figure}


\section{Discussion}

We have examined morpheme order in nouns and verbs in six morphologically rich and agglutinating languages, testing the recently propiosed Efficient Tradeoff Hypothesis \citep{hahn2020modeling} as an explanatory account of morpheme ordering.
We compared actual morpheme orderings to other possible orderings and to orderings optimized for efficiency of the memory-surprisal tradeoff.
In most cases, we found that the real ordering provided more efficient tradeoffs than most alternative orderings.
More importantly, we found that the real orderings match the optimal orderings in many respects.
Ordering forms found in a corpus according according to real or optimized orderings yields the same orders in XXX of cases.
In some cases, particularly for noun inflection, match between real and optimized orderings is perfect.
Beyond language-specific ordering patterns, optimization recovers previously-documented language universals of morpheme order. \jd{These results support...}

\subsection{Relation to previous accounts}

%\subsubsection{Semantic Scope; Syntactic Structure; Historical Development}
%Morpheme ordering has been described from the perspective 
%Besides the explanation of broad typological tendencies, which we are interested in here, the linguistic literature has also debated at which level of linguistic representation morpheme ordering should be described, proposing accounts located at different levels such as the syntax-phonology interface \citep{baker1985the} and an autonomous layer of morphological description \citep{hyman2003suffix}.
%TODO also template vs scope vs...
%We do not see the memory--surprisal tradeoff as competing with or contradicting these studies.
%Rather, it specifies tendencies for ordering that could be implemented at different levels of linguistic description.

%\paragraph{Revelance and Proximity}


%A related family of explanations holds that those affixes are closest to the root that are most relevant to the root.
%Our optimized orders of Turkish and Hungarian \becky{Are there other languages?} perfectly match Bybee's proposed universal verbal inflection order (Figure REF).
%This suggests that high mutual information generally correlates with semantic relevance. 


In this section, we relate our results to existing explanatory accounts of morpheme ordering.
In a review of research on morpheme ordering, \cite{manova2010modeling} % https://homepage.univie.ac.at/stela.manova/modeling%20affix%20order.pdf
categorizes approaches to morpheme ordering into three classes: orderings that are grammatically motivated by properties of syntac, semantics or phonology; orderings that extra-grammatically motivated by statistical properties of language and human cognition; and orderings that are arbitrarily stipulated.
Our approach falls into the second class, explaining morpheme ordering based on minimization of human processing effort.
In this section, we describe how our account relates to other accounts across these three clusters, and show how the memory-surprisal tradeoff has tight connections with notions proposed across seemingly very different accounts.

\subsection{Grammatical Motivations}



%\mhahn{report the following somewhere}
%We measured the mutual information between roots and features using the other Universal Dependencies corpora for which these features were available.
%First, for nouns, we compared the mutual information between the lemma and case and number, using a mixed-effects model with random effects for languages and language families.
%Across 34 languages, mutual information tended to be higher for number than for case ($\beta=0.04$, $SE=0.015$, $t=2.58$, 30 languages).
%For verbs, we compared mutual information between the root and voice, TAM, and subject agreement features.
%Mutual information was higher for TAM than for agreement ($\beta=0.1$, $SE=0.04$, $t=2.29$, 37 languages), however, there was no significant difference between TAM and voice ($\beta=0.08$, $SE=0.058$, $t=1.39$, 30 languages).


%TODO think about where to explain this As mentioned in section \becky{cite S2}, Bybee proposes a concept of \textit{semantic relevance}, which is the degree to which one morpheme affects the semantic content of the other \becky{cite bybee}. She hypothesizes that morphemes that are more highly relevant to each other should be closer to each other, and therefore proposes a universal ordering of verbal inflectional morphemes. For example, attaching a causative marker to a root meaning "to die" produces a verb form meaning "to kill." Since the causative greatly changes the semantic content of the original root, the causative is highly relevant to the root. 
%\becky{Format Korean example of juk-da ``to die" and juk-i-da ``to kill"}
%Bybee did not provide a way to computationally measure semantic relevance. 


%Other explanations suggest that morphemes are ordered based on (TODO CITE).
%The most straightforwardly related account is that of Bybee (1985) Semantic relevance
%CARP template in Bantu
%Explanations for the orders

%talk about Relevance and Mutual Information
%\paragraph{Morphemes as Fossilized Words}\

It has long been observed that the order of morphemes often parallels the order of independent words of corresponding meanings \citep{givon1971historical,venneman1973explanation,baker1985the}.
One explanation for this is that the order of morphemes reflects the order of formerly independent elements that have been fossilized into bound morphemes, which can often be verified in languages where historical data is available \citet{givon1971historical,venneman1973explanation}.
% interesting: Evans, Nicholas (1995). Multiple case in Kayardild: Anti-iconic suffix ordering and the diachronic filter. In Plank (ed.) 1995, 396-430. 
% Moravcsik, Edith A. (1995). Summing up Suffixaufnahme. In Plank (ed.) 1995, 451-484.  HEREL : 478, G24
% Cited by https://typo.uni-konstanz.de/rara/universals-archive/41/
On the other hand, \citet{bybee-morphology-1985} points out that there are historically documented cases where morpheme ordering has been restructured in ways that do not reflect former independent words, but respect the universal tendencies proposed by her (see also \citet{mithun2000the, haspelmath1993the, mithun1995affixation}; \citet[Section 15]{rice2000morpheme}).
A correspondence between the order of words and morphemes has also been postulated on a purely synchronic basis as a constraint on possible human languages.
For example, \cite{baker1985the} proposed the Mirror Principle, which -- informally -- states that the order of elements (morphemes) in morphology reflects the order of elements (words) in syntax.
A large body of work in theoretical syntax describes word and morpheme order using the same processes and principles \citep{halle1993distributed}.
As a theory of order at multiple levels, the Efficient Tradeoff Hypothesis is compatible with these different historical paths and predicts these ordering patterns independently of the historical path leading to the morphemes found in a given language.



%As the memory-surprisal tradeoff is optimized by word order, this is compatible with our results:
%To the extent that morpheme order does reflect fossilized word order, morpheme order should continue to reflect optimization for the tradeoff.


%To the extent that similar statistical patterns of usage hold for corresponding words and morphemes, this again is compatible with the Efficient Tradeoff Hypothesis.

%\mhahn{TODO regarding scope: \citep{baker1988incorporation,foley1984functional,chierchia1990meaning,valin1992a}}


%This can be related to the scope-based explanation to the extent that syntactic structure and word order reflect scope.
% INTERESTING reference: http://www.jzeller.de/pdf/CARP.pdf

%\paragraph{Semantic Scope}


%Description in terms of scope faces limitations when order is different from semantic scope.
%\cite{Hyman2003}
%Hyman  (2003:  249) proposed the CARP template to describe suffix order in Bantu languages (such as Sesotho), a templatic order for valence and voice morphemes that can be different from scope order.
%In \cite{Hyman2003}'s model, order is determined based on both the CARP template and a preference for ismorphism with semantic scope.
%

Another prominent account is in terms of semantic scope \citep{rice2000morpheme}.
This is the idea that morphemes are ordered in the order in which their meanings combine.
The relative ordering of valence and voice is a good example for the scope-based explanation.
Valence affixes change the number of arguments, and passive (voice) promotes one argument to the subject position.
This can affect an argument introduced by a voice marker.
The following example from Turkish illustrates this (\cite{schaaik2020turkish}, section 30.8.2):


\begin{tabular}{ccccccc}
don-dur-ul & freeze-caus-pass & be frozen \\
don-dur & freeze-caus & to freeze (something) \\
don & freeze& to freeze \\
\end{tabular}

Turkish has suffixes both for causative and passive.
When adding both suffixes simultaneously, the causative marker appears closer to the root.
This corresponds to the order in which the meanings of these suffixes combine with the meaning of the root:
Causative adds an argument indicating who makes an object freeze, and passive then removes that argument, yielding a verb describing something that is being frozen by someone.

A priori, semantic scope is different from mutual information.
However, there are arguments that scope relations correlate with mutual information.
\citet{culbertson2020from} made a related argument for the order of nominal modifiers as in ``the four green books'', where mutual information predicts the order of attachment of modifiers to the noun.
Furthermore, if the semantic propositions that speakers communicate are modeled using a probabilistic language of thought grammar such as CITE, mutual information will also correlate with scope ordering (see SI Appendix Section X for a simulation study).


\cite{Hahn2020modeling} suggested that information locality provides a formalization of Bybee's \citep{bybee-morphology-1985} claim that semantic relevance determines ordering.
We suggest that mutual information provides an operational formalization of semantic relevance.
Relatedly, \citep{culbertson2020from} link conceptual closeness to MI, and argue that it is a driver of order (in the domain of noun phrase modifiers)

\subsection{Cognitive Motivations}


Usage statistics

-- Hay 2002

-- Plag 2002

-- Hay and Plag 2004

-- Hay and Baayen 2005

-- Plag and Baayen 2009: productivity

all show that `less parsable' affixes are closer to root



Inkelas: informativity: they propose that bigram surprisal is minimized (not their terminology), test this in a pilot study of Turkish, with partially confirming results \citep{inkelas2015informativity}


\subsection{Arbitrary Orderings}

also discuss \cite{ryan2010variable}: weighted bigram constraints in Tagalog, for the (rather uncommon) case of variable/flexible morpheme ordering

\cite{inkelas2016affix}: optimality-thepretic model of lexiacl production, integrates semantic relevance (Bybee) and Scope (Baker, Rice).

\citet{hay2004what} propose a model of complexity-based ordering based on processing considerations, called complexity-based ordering, arguing that affixes are further away from the root when they can be separated more easily in processing, testing this theory using 15 English affixes.


from Inkelas

- citing Rice 2011; Manova and Aronoff 2010; Saarinen and Hay 2014

Rice states three types of explanations/phenomena:

- grammatuical principle (syntactic, semantic, phonological). famous examples Mirror Principle and Bybee's relevance Principle; Scope (Rice)

- arbitrary language-specific tenplatic (Fabb 1988 English affix ordering; position class templates, Simpson and Withgott 1986) (also finite-state automata Hankamer 1986; Karttunen 2003) \cite{fabb1988english}

- extra-grammatical: frequency, productivity, parsability

Mirror princuiple built into Distributed Morphology (Harley and Noyer 1998)

Tension between Scope and templatic: Hyman 2003; Aronoff and Xu 2010; Spencer 2013

re scope also Alsina 1999

\cite{manova2010modeling} % https://homepage.univie.ac.at/stela.manova/modeling%20affix%20order.pdf
%
also provide a ctagoeirzation into grammatical, psycholinguistic, unmotivated (among others)

\cite{aronoff2010introduction}

\cite{spencer2003putting}

\cite{alsina1999where}

\cite{aronoff2010a} % Within this model, phonological form is spelled out by means of individual-language-particular realization constraints that associate abstract morphosyntactic feature values with phonological forms and that are ordered among more general constraints governing factors like scope and feature splitting. The data used to exemplify the application of our theory to affix order are drawn from Haspelmath’s (A grammar of Lezgian, Mouton de Gruyter, Berlin, 1993) grammar of Lezgian, a language of the Northeast Caucasian family spoken largely in Dagestan (Russia) and Azerbaijan.




\cite{plag2009suffix} %http://www.sfs.uni-tuebingen.de/~hbaayen/publications/plagBaayenLanguage2009.pdf



%\paragraph{Templates}
%CARP template
%\cite{muysken1981quechua}
%\cite{mccarthy2008generalized} phonology
%\cite{hyman2003suffix}
%\cite{kanu2009suffix} morphotactics, not semantic scope







\subsection{Other Aspects of Morphology}

Our study focused on agglutination, where a word carries multiple clearly separated morphemes with distinct functions.
There are other types of morphological processes that deserve study.

Many languages show fusion where different categories are fused into a single morpheme, or stem changes, such as English swim $\rightarrow$ swam.
An extreme case is non-concatenative morphology (in Arabic, k-t-b `to write' forms katab- `wrote'. -aktub `write/be writing', -kutib- `was written').
These types of morphological processes are not described in terms of the order of different morphemes.


While we have focused on the relative distance from the root, we have not touched on the question of why a morpheme is realized as a prefix or a suffix in a given language.
There are well-known correlations between suffixing or prefixing preference and word order \citep{greenberg1963universals}.




\subsection{Limitations}

- genre of data: written. but Sesotho is spoken (child-directed)

- limitations of computational estimation of the memory-surprisal tradeoff. %However: As the set of all moprphological forms of a language is typically finite (CITE), we believe that these limitations play a smaller role in morpheme order, compared to word order.


A further limitation relates to the selection of languages.
The six languages considered in this study are spoken in Eurasia and Africa, not representing Australia and America.
Among the languages, Hungarian and Finnish are genetically related (X millenia).
There is also some evidence for genetic relations beyond these (Japanese, Korean, Turkish), but such relations would have to be quite ancient (x millenia).
The morphemes found in these languages as considered here are generally not cognate.
Thus, the commonalities in languages found cannot be traced to inherited orderings of morphemes inherited from a common ancestor.
Furthermore, we note that the optimality of different orderings crucially depend not on the morphemes themselves (which might be inherited), but on the frequencies of different forms (e.g., of sinulars and plurals).



\section{Conclusion}

We have tested the recently proposed Efficient Tradeoff Hypothesis as a predictor of morpheme order with data from verbs and nouns across six languages.
We found that attested morpheme orders provide more efficient tradeoffs than most other possible orderings and that many properties of observed orderings are recovered by optimizing for tradeoff efficiency.
With the exception of verb inflection in Finnish and Sesotho, we found that optimized orderings agree with the attested orderings in more than 90\% of forms found in corpora.
Optimization also successfully predicts several universals of morpheme ordering, both for nouns and verbs. \jd{add concluding sentence tying all together}


\printbibliography

\end{document}

